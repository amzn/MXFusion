{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Gaussian Process\n",
    "\n",
    "### Author: Mark Pullin\n",
    "\n",
    "This notebooks demonstrates the use of deep Gaussian processes in MXFusion for regression. The model is based on the [Doubly Stochastic Variational Inference for Deep Gaussian Processes (Hugh Salimbeni, Marc Deisenroth)](https://arxiv.org/abs/1705.08933) paper.\n",
    "\n",
    "This notebook fits the model to the [\"kin8nm\" dataset](https://www.openml.org/d/189)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['MXNET_ENGINE_TYPE'] = 'NaiveEngine'\n",
    "FIG_SIZE = (12, 8)\n",
    "\n",
    "import mxfusion\n",
    "from mxfusion.common import config\n",
    "import mxnet as mx\n",
    "dtype = 'float64'\n",
    "config.DEFAULT_DTYPE = dtype\n",
    "\n",
    "context = mx.gpu()\n",
    "config.MXNET_DEFAULT_DEVICE = context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get training data\n",
    "\n",
    "The training data will be downloaded from UCI repo, then the data is split into training and test set. 10% of the data is reserved for testing. The features and targets are then normalized by the mean and standard deviation of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data_path = 'https://www.openml.org/data/get_csv/3626/dataset_2175_kin8nm.arff'\n",
    "\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "X = df.values[:, :-1]\n",
    "Y = df.values[:, [-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do train/test split and normalize data set with mean and standard deviation of training data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.1)\n",
    "\n",
    "x_std = x_train.std(axis=0)\n",
    "x_mean = x_train.mean(axis=0)\n",
    "y_std = y_train.std(axis=0)\n",
    "y_mean = y_train.mean(axis=0)\n",
    "\n",
    "x_train = (x_train - x_mean)/x_std\n",
    "x_test = (x_test - x_mean)/x_std\n",
    "\n",
    "y_train = (y_train - y_mean)/y_std\n",
    "y_test = (y_test - y_mean)/y_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make DGP model\n",
    "\n",
    "This DGP has 3 layers, with hidden layers having dimensionality of 30. Each layer has 100 inducing points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 8\n",
      "8 8\n",
      "8 1\n",
      "(8, 1)\n"
     ]
    }
   ],
   "source": [
    "import mxnet as mx\n",
    "import numpy as np\n",
    "import mxfusion.components.distributions.gp.kernels\n",
    "from mxfusion.components.variables import PositiveTransformation\n",
    "from mxfusion.modules.gp_modules.deep_gp_regression import DeepGPRegression\n",
    "from mxfusion.components.distributions.gp.kernels import RBF, White\n",
    "from scipy.cluster.vq import kmeans2\n",
    "from functools import partial\n",
    "\n",
    "D = x_train.shape[1]\n",
    "\n",
    "def make_mean_and_Z(n_dimensions, X, Z):\n",
    "    \"\"\"\n",
    "    Function to create mean functions and inducing point locations for layers\n",
    "    \n",
    "    :param n_dimensions: List of length n_layers + 1 containing input/output dimensionality for each layer\n",
    "    :param X: training data\n",
    "    :param Z: inducing points at first layer\n",
    "    \"\"\"\n",
    "    dtype = 'float64'\n",
    "    \n",
    "    def mx_dot(x, y):\n",
    "        return mx.nd.dot(y, x)\n",
    "    \n",
    "    n_layers = len(n_dimensions) - 1\n",
    "    \n",
    "    X_running = mx.nd.array(X, dtype=dtype, ctx=mx.cpu())\n",
    "    Z_running = mx.nd.array(Z, dtype=dtype, ctx=mx.cpu())\n",
    "    \n",
    "    mean_fcns = []\n",
    "    inducing_points = []\n",
    "    for i in range(n_layers):\n",
    "        inducing_points.append(Z_running)\n",
    "        input_dim = n_dimensions[i]\n",
    "        output_dim = n_dimensions[i+1]\n",
    "        \n",
    "        print(input_dim, output_dim)\n",
    "        if input_dim == output_dim:\n",
    "            mean_mat = mx.nd.eye(input_dim, dtype=dtype, ctx=context)\n",
    "        elif input_dim > output_dim:\n",
    "            _, _, v = np.linalg.svd(X_running.asnumpy(), full_matrices=False)\n",
    "            mean_mat = mx.nd.array(v, dtype=dtype, ctx=context)[:output_dim, :].T\n",
    "            print(mean_mat.shape)\n",
    "        else:\n",
    "            # output_dim > input_dim\n",
    "            padding_array = mx.nd.zeros((input_dim, output_dim - input_dim), dtype=dtype, ctx=context)\n",
    "            mean_mat = mx.nd.concatenate([mx.nd.eye(input_dim, ctx=context, dtype=dtype), padding_array], axis=1)\n",
    "        X_running = mx.nd.dot(X_running, mean_mat.as_in_context(mx.cpu()))\n",
    "        Z_running = mx.nd.dot(Z_running, mean_mat.as_in_context(mx.cpu()))\n",
    "        \n",
    "        mean_fcns.append(partial(mx_dot, mean_mat))\n",
    "        \n",
    "    return mean_fcns, inducing_points\n",
    "\n",
    "def make_dgp_model(n_dimensions, n_inducing, Z, mean_functions):\n",
    "    \"\"\"\n",
    "    Makes a DGP model with specified latent dimensions with RBF kernels at each layer\n",
    "    \"\"\"\n",
    "    layer_input_dimensions = n_dimensions[:-1]\n",
    "    layer_output_dimensions = n_dimensions[1:]\n",
    "    \n",
    "    m = mxfusion.Model()\n",
    "    m.N = mxfusion.Variable()\n",
    "    \n",
    "    n_layers = len(layer_input_dimensions)\n",
    "    m.X = mxfusion.Variable(shape=(m.N, n_dimensions[0]))\n",
    "    kernels = []\n",
    "    for i in range(n_layers):\n",
    "        M = n_inducing[i]\n",
    "        kernels.append(RBF(layer_input_dimensions[i], lengthscale=2., variance=2., ctx=context, ARD=True) \n",
    "                      + White(layer_input_dimensions[i], variance=1e-5))\n",
    "        z_locations = np.linspace(-1, 1, M)[:, None]\n",
    "        setattr(m, 'Z_' + str(i), mxfusion.Variable(shape=(M, layer_input_dimensions[i]), \n",
    "                                                    initial_value=mx.nd.array(Z[i])))\n",
    "        \n",
    "    m.noise_var = mxfusion.Variable(transformation=PositiveTransformation())\n",
    "    inducing_variables = [getattr(m, 'Z_' + str(i)) for i in range(n_layers)]\n",
    "    m.Y = DeepGPRegression.define_variable(m.X, kernels, m.noise_var, shape=(m.N, n_dimensions[-1]), \n",
    "                                           inducing_inputs=inducing_variables, n_samples=5, \n",
    "                                           mean_functions=mean_functions)\n",
    "    \n",
    "    return m\n",
    "\n",
    "Z_100 = kmeans2(x_train, 100, minit='points')[0]\n",
    "\n",
    "M = 10\n",
    "\n",
    "n_layers = 3\n",
    "n_inducing = [100] * n_layers\n",
    "n_dims_hidden_layers = min(D, 30)\n",
    "n_dimensions = [D] + [n_dims_hidden_layers] * (n_layers - 1) + [1]\n",
    "mean_fcns, inducing_locations = make_mean_and_Z(n_dimensions, x_train, Z_100)\n",
    "m = make_dgp_model(n_dimensions, n_inducing, inducing_locations, mean_fcns)\n",
    "n_layers = m.Y.factor.n_layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create inference object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxfusion.inference import GradBasedInference, MAP, MinibatchInferenceLoop\n",
    "\n",
    "batch_size = min(10000, x_train.shape[0]/2)\n",
    "infr = GradBasedInference(inference_algorithm=MAP(model=m, observed=[m.X, m.Y]), \n",
    "                          grad_loop=MinibatchInferenceLoop(batch_size), dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/git/MXFusion/mxfusion/inference/inference_parameters.py:71: UserWarning: InferenceParameters has already been initialized.  The existing one will be overwritten.\n",
      "  warnings.warn(\"InferenceParameters has already been initialized.  The existing one will be overwritten.\")\n"
     ]
    }
   ],
   "source": [
    "infr.initialize(X=x_train.shape, Y=y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize parameter values\n",
    "\n",
    "The variational distribution at each layer is initialized to have zero mean, identity covariance matrix scaled by 1e-5. The noise variance is initialized to 0.01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/git/MXFusion/mxfusion/inference/inference.py:163: UserWarning: Trying to initialize the inference twice, skipping.\n",
      "  warnings.warn(\"Trying to initialize the inference twice, skipping.\")\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "def initialise_model(m, infr):\n",
    "    \"\"\"\n",
    "    Initializes mean of variational distribution at inducing points to be identity function\n",
    "    \"\"\"\n",
    "    dtype = 'float64'\n",
    "    infr.initialize(X=mx.nd.array(X, dtype=dtype, ctx=context), \n",
    "                    Y=mx.nd.array(Y, dtype=dtype, ctx=context))\n",
    "    \n",
    "    def mx_dot(x, y):\n",
    "        return mx.nd.dot(y, x)\n",
    "    \n",
    "    mean_fcns = []\n",
    "    for i in range(n_layers):\n",
    "        mean = getattr(m.Y.factor._extra_graphs[0], 'qU_mean_' + str(i))\n",
    "        cov_w = getattr(m.Y.factor._extra_graphs[0], 'qU_cov_W_' + str(i))\n",
    "        cov_diag = getattr(m.Y.factor._extra_graphs[0], 'qU_cov_diag_' + str(i))\n",
    "        infr.params[cov_w] = mx.nd.zeros_like(infr.params[cov_w])\n",
    "        infr.params[cov_diag] = mx.nd.ones_like(infr.params[cov_diag]) * 1e-5\n",
    "        infr.params[mean] = mx.nd.zeros_like(infr.params[mean])\n",
    "    infr.params[m.noise_var] = mx.nd.array([0.01], ctx=context)\n",
    "initialise_model(m, infr)\n",
    "m.Y.factor.dgp_log_pdf.jitter = 1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/git/MXFusion/mxfusion/inference/inference.py:163: UserWarning: Trying to initialize the inference twice, skipping.\n",
      "  warnings.warn(\"Trying to initialize the inference twice, skipping.\")\n"
     ]
    }
   ],
   "source": [
    "infr.run(X=mx.nd.array(x_train, dtype='float64', ctx=context), \n",
    "         Y=mx.nd.array(y_train, dtype='float64', ctx=context), max_iter=2000, learning_rate=0.01)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict from model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/git/MXFusion/mxfusion/inference/inference_parameters.py:71: UserWarning: InferenceParameters has already been initialized.  The existing one will be overwritten.\n",
      "  warnings.warn(\"InferenceParameters has already been initialized.  The existing one will be overwritten.\")\n"
     ]
    }
   ],
   "source": [
    "m.Y.factor.dgp_predict.noise_free = False\n",
    "from mxfusion.inference import ModulePredictionAlgorithm, TransferInference\n",
    "infr_pred = TransferInference(ModulePredictionAlgorithm(model=m, observed=[m.X], target_variables=[m.Y]), \n",
    "                              infr_params=infr.params)\n",
    "\n",
    "res = infr_pred.run(X=mx.nd.array(x_test, dtype='float64', ctx=context))\n",
    "\n",
    "mean_samples, var_samples = res[0][0].asnumpy(), res[0][1].asnumpy()\n",
    "\n",
    "mean = mean_samples.mean(axis=0)\n",
    "var = (var_samples + mean_samples**2).mean(axis=0) - mean**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = np.sqrt(np.mean(np.square((mean - y_test)))) * y_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "test_log_likelihood = scipy.stats.norm.logpdf(y_test * y_std, mean * y_std, np.sqrt(var) * y_std).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE; 0.0681\n",
      "Log likelihood: 1.2289\n"
     ]
    }
   ],
   "source": [
    "print('RMSE; {:.4f}'.format(rmse[0]))\n",
    "print('Log likelihood: {:.4f}'.format(test_log_likelihood))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare against single layer SVGP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "svgp_model = mxfusion.Model()\n",
    "svgp_model.N = mxfusion.Variable()\n",
    "svgp_model.X = mxfusion.Variable(shape=(svgp_model.N, D))\n",
    "svgp_model.Z= mxfusion.Variable(initial_value=mx.nd.array(Z_100, dtype=dtype, ctx=context), shape=(100, D))\n",
    "svgp_model.noise_var = mxfusion.Variable(transformation=PositiveTransformation(), initial_value=0.01)\n",
    "kernel = RBF(D, ARD=True)\n",
    "svgp_model.Y = mxfusion.modules.gp_modules.SVGPRegression.define_variable(svgp_model.X, kernel, svgp_model.noise_var, \n",
    "                                                                          shape=(svgp_model.N, 1), \n",
    "                                                                          inducing_inputs=svgp_model.Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/git/MXFusion/mxfusion/inference/inference_parameters.py:71: UserWarning: InferenceParameters has already been initialized.  The existing one will be overwritten.\n",
      "  warnings.warn(\"InferenceParameters has already been initialized.  The existing one will be overwritten.\")\n"
     ]
    }
   ],
   "source": [
    "svgp_infr = GradBasedInference(inference_algorithm=MAP(model=svgp_model, observed=[svgp_model.X, svgp_model.Y]), \n",
    "                          grad_loop=MinibatchInferenceLoop(batch_size), dtype=dtype)\n",
    "svgp_infr.initialize(X=x_train.shape, Y=y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/git/MXFusion/mxfusion/inference/inference.py:163: UserWarning: Trying to initialize the inference twice, skipping.\n",
      "  warnings.warn(\"Trying to initialize the inference twice, skipping.\")\n"
     ]
    }
   ],
   "source": [
    "svgp_infr.run(X=mx.nd.array(x_train, dtype='float64', ctx=context), \n",
    "              Y=mx.nd.array(y_train, dtype='float64', ctx=context), max_iter=2000, learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/git/MXFusion/mxfusion/inference/inference_parameters.py:71: UserWarning: InferenceParameters has already been initialized.  The existing one will be overwritten.\n",
      "  warnings.warn(\"InferenceParameters has already been initialized.  The existing one will be overwritten.\")\n"
     ]
    }
   ],
   "source": [
    "svgp_infr_pred = TransferInference(ModulePredictionAlgorithm(model=svgp_model, observed=[svgp_model.X], \n",
    "                                                             target_variables=[svgp_model.Y]), \n",
    "                                   infr_params=svgp_infr.params)\n",
    "\n",
    "svgp_model.Y.factor.svgp_predict.noise_free = False\n",
    "res = svgp_infr_pred.run(X=mx.nd.array(x_test, dtype='float64', ctx=context))\n",
    "\n",
    "mean, var = res[0][0].asnumpy(), res[0][1].asnumpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE; 0.0876\n",
      "Log likelihood: 0.9679\n"
     ]
    }
   ],
   "source": [
    "rmse = np.sqrt(np.mean(np.square((mean - y_test)))) * y_std\n",
    "test_log_likelihood = scipy.stats.norm.logpdf(y_test * y_std, mean * y_std, np.sqrt(var) * y_std).mean()\n",
    "\n",
    "print('RMSE; {:.4f}'.format(rmse[0]))\n",
    "print('Log likelihood: {:.4f}'.format(test_log_likelihood))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the DGP outperforms the single layer GP for this dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_mxnet_p36)",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
