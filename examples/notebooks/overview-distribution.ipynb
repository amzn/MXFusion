{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview of MXFusion's Distributed Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n",
    "#\n",
    "#   Licensed under the Apache License, Version 2.0 (the \"License\").\n",
    "#   You may not use this file except in compliance with the License.\n",
    "#   A copy of the License is located at\n",
    "#\n",
    "#       http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "#   or in the \"license\" file accompanying this file. This file is distributed\n",
    "#   on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either\n",
    "#   express or implied. See the License for the specific language governing\n",
    "#   permissions and limitations under the License.\n",
    "# ==============================================================================\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MXFusion uses Horovod as its distributed training framework. Horovod is Uber’s Open Source Distributed Deep Learning Framework. It provides support for various deep learning libraries, which are Tensorflow, Keras, PyTorch and Apache MXNet. MXFusion, which is built on top of MXNet, benefits from Horovod's high-level API that makes distributed deep learning easily implemented. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Horovod uses Facebook’s data parallel approach to distributed training. The data-parallel distributed training procedures consists of 3 stages. \n",
    "\n",
    "1. Run multiple copies (depends on number of processors) of the training script which takes a chunk of the data and computes the gradient by running it through the designed model.\n",
    "2. Averages the gradients among multiple copies.\n",
    "3. Update the model. Repeat Step 1 until all iterations are completed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Horovod's core principles are based on the MPI concepts. One concept is <tt>Allreduce</tt>, which aggregates data among multiple processes and sends back to each processes. Its [DistributedTrainer](https://github.com/horovod/horovod/blob/master/horovod/mxnet/__init__.py) API is a subclass of MXNet gluon.Trainer, which averages data after <tt>Allreduce</tt> gradients among processors. We are able to utilize this high-level API in training our probabilistic models in MXFusion, as long as the loss function equation below is valid:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "L = \\left( \\sum_{k=1}^n L_k \\right)\n",
    "\\end{equation*}\n",
    "\n",
    "where $L$ is the final loss function and $L_k$ is the loss function of processor k."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation on MXFusion "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implemented very simple interface to call MXFusion in distributed training in high level. We introduced <tt>DistributedGradBasedInference</tt>, which is the distributed class of <tt>GradBasedInference</tt>. The only additional method of this class is <tt>rescale</tt>, which rescales the latent variables of SVI based on number of processors running.\n",
    "\n",
    "In SVI, the loss functions takes in the mean and variance of prior and posterior of each latent variables into account. Summing the loss functions of each processors would cause multiplication those mean and variances. Hence, one method to return the correct final loss function is rescaling the variables based on number of processors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rescale(self, rv_scaling):\n",
    "    \"\"\"\n",
    "    Return the rescaled scaling factor of random variables for SVI.\n",
    "    \"\"\"\n",
    "    import horovod.mxnet as hvd\n",
    "    for _, variable in enumerate(\n",
    "            self.inference_algorithm.model.get_latent_variables(self.inference_algorithm.observed)):\n",
    "        if variable not in rv_scaling:\n",
    "            rv_scaling[variable.uuid] = 1 / hvd.size()\n",
    "\n",
    "    return rv_scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to <tt>GradBasedInference</tt>, <tt>DistributedGradBasedInference</tt> runs the training in a batch gradient-based optimization recursively. While <tt>GradBasedInference</tt> uses <tt>BatchInferenceLoop</tt> and <tt>MinibatchInferenceLoop</tt>, <tt>DistributedGradBasedInference</tt> uses <tt>DistributedBatchInferenceLoop</tt> and <tt>DistributedMinibatchInferenceLoop</tt>. Both of the batch loops inherit <tt>DistributedGradLoop</tt>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As Horovod runs on multiple scripts, which require each scripts to take a different partition of data. <tt>DistributedGradLoop</tt> has the method <tt>split_data</tt> to allow each processors. Since Horovod is based on MPI concepts, we can make use of <tt>size</tt> (total number of processors) and <tt>rank</tt> (unique ID of each processors) to determine which part and how big the partition should be taken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(self, data):\n",
    "    if hvd.size() > 1:\n",
    "        temporaryData = []\n",
    "        \n",
    "        for _, subdata in enumerate(data):\n",
    "            x = int(subdata.shape[0] / hvd.size())\n",
    "            y = subdata.shape[0] % hvd.size()\n",
    "            rank = hvd.rank()\n",
    "            z = 0 if (rank < y) else 1\n",
    "            f = 0 if (rank < y + 1) else 1\n",
    "            start_point = rank*x+rank-f*(rank-y)\n",
    "            end_point = (rank+1)*x+rank-z*(rank-y+1) + 1\n",
    "            tempData = mx.nd.slice_axis(subdata, axis=0, begin=start_point, end=end_point)\n",
    "            temporaryData.append(tempData)\n",
    "\n",
    "        data = temporaryData\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In <tt>DistributedBatchInferenceLoop</tt> and <tt>DistributedMinibatchInferenceLoop</tt>, the trainer is created using Horovod's <tt>DistributedTrainer</tt> while passing in the same parameters, optimizer and optimizer's parameters. We also split the data with <tt>split_data</tt> method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = hvd.DistributedTrainer(param_dict, optimizer=optimizer,optimizer_params={'learning_rate': learning_rate})\n",
    "data = self.split_data(data=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When each processors has executed one iteration, the total iteration executed is $n$ times, where $n$ is the number of processors. With the same learning rate, the processors should move to the negative gradient with step size of $n$ times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_for_gradient = loss_for_gradient * hvd.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next tutorial will demonstrate on how to use the implemented MXFusion's APIs to run distributed training of probabilistic models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
